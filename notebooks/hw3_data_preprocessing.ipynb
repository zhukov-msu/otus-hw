{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f425290b-3c66-4b97-b5fa-57f2ed5b2096",
   "metadata": {},
   "source": [
    "# Очистка датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617bac62-9651-44db-b80b-4b736fc80e67",
   "metadata": {},
   "source": [
    "## Imports & inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "698e6aa5-a245-4b79-b44e-10079a03f730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0524087-ac08-4192-9083-78e55dc5b7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = \"python\"\n",
    "# os.environ['PYSPARK_PYTHON'] = \"../.venv/bin/python\"\n",
    "# os.environ['PYSPARK_PYTHON'] = \"/home/ubuntu/otus-hw/.venv/bin/python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c89ec26-7304-4e63-b407-4abf6f259ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "conf = (\n",
    "    SparkConf().setMaster(\"yarn\").setAppName(\"data_cleaning\")\n",
    "        .set(\"spark.executor.memory\", \"8g\")\n",
    "        .set(\"spark.driver.memory\", \"4g\")\n",
    "        .set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        .set(\"spark.executor.instances\", 4)\n",
    "        .set(\"spark.executor.cores\", 2)\n",
    "        .set('spark.sql.repl.eagerEval.enabled', True) \n",
    ")\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed20a1dd-693f-4805-bc71-f2643ef8126a",
   "metadata": {},
   "source": [
    "## Get data & preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0225221-1d0e-49c0-a3ed-875018c70b38",
   "metadata": {},
   "source": [
    "### HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9302ee-32e0-4737-82bd-c5db3ad08d4c",
   "metadata": {},
   "source": [
    "В данном моменте файлы уже загружены в HDFS из бакета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2efcd2b1-b67a-4c3d-93f4-6ca756f0893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfsdir = '/user/data'\n",
    "target_s3 = 's3a://hw3-data-cleaning/'\n",
    "RESULT_BUCKET = \"hw3-data-cleaning\"\n",
    "INPUT_BUCKET = \"otus-hw-bucket\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d7e26338-2143-4125-8589-d59e487fc807",
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = [ line.rsplit(None,1)[-1] for line in sh.hdfs('dfs','-ls',hdfsdir).split('\\n') if len(line.rsplit(None,1))][1:]\n",
    "filelist = [f for f in filelist if f.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1a6e7942-6413-4eed-bd3c-58afeac7b4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/user/data/2019-08-22.txt',\n",
       " '/user/data/2019-09-21.txt',\n",
       " '/user/data/2019-10-21.txt',\n",
       " '/user/data/2019-11-20.txt',\n",
       " '/user/data/2019-12-20.txt']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filelist[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "53ce364c-392c-4695-9d8c-db81acb3c5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([StructField(\"transaction_id\", LongType(), True),\n",
    "                     StructField(\"tx_datetime\", TimestampType(), True),\n",
    "                     StructField(\"customer_id\", LongType(), True),\n",
    "                     StructField(\"terminal_id\", LongType(), True),\n",
    "                     StructField(\"tx_amount\", DoubleType(), True),\n",
    "                     StructField(\"tx_time_seconds\", LongType(), True),\n",
    "                     StructField(\"tx_time_days\", LongType(), True),\n",
    "                     StructField(\"tx_fraud\", LongType(), True),\n",
    "                     StructField(\"tx_fraud_scenario\", LongType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "61fa0d26-dee7-4f6f-a523-c88f8dea7fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d84a26e6-6a94-43a0-a1fa-8f6ae8a396d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = filelist[0]\n",
    "filename_clean = filelist[0].split('/')[-1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "32b2e516-2fbf-40f8-b211-75aa1099124c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file name in hdfs: /user/data/2019-08-22.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"file name in hdfs:\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "64065d25-30fa-43d3-b0ed-d4857c526911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file name in to write: 2019-08-22\n"
     ]
    }
   ],
   "source": [
    "print(\"file name in to write:\", filename_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "88433e79-54d6-4595-ad5b-95c36e84483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.schema(schema)\\\n",
    "                .option(\"comment\", \"#\")\\\n",
    "                .option(\"header\", False)\\\n",
    "                .csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9574b413-deea-4a3d-974f-aa0f6597ff8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|transaction_id|        tx_datetime|customer_id|terminal_id|tx_amount|tx_time_seconds|tx_time_days|tx_fraud|tx_fraud_scenario|\n",
      "+--------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|             0|2019-08-22 06:51:03|          0|        711|    70.91|          24663|           0|       0|                0|\n",
      "|             1|2019-08-22 05:10:37|          0|          0|    90.55|          18637|           0|       0|                0|\n",
      "|             2|2019-08-22 19:05:33|          0|        753|    35.38|          68733|           0|       0|                0|\n",
      "|             3|2019-08-22 07:21:33|          0|          0|    80.41|          26493|           0|       0|                0|\n",
      "|             4|2019-08-22 09:06:17|          1|        981|   102.83|          32777|           0|       0|                0|\n",
      "|             5|2019-08-22 18:41:25|          3|        205|     34.2|          67285|           0|       0|                0|\n",
      "|             6|2019-08-22 03:12:21|          3|          0|     47.2|          11541|           0|       0|                0|\n",
      "|             7|2019-08-22 22:36:40|          6|        809|   139.39|          81400|           0|       0|                0|\n",
      "|             8|2019-08-22 17:23:29|          7|        184|    87.24|          62609|           0|       0|                0|\n",
      "|             9|2019-08-22 21:09:37|          8|        931|     61.7|          76177|           0|       0|                0|\n",
      "|            10|2019-08-22 11:32:42|         10|        663|    40.71|          41562|           0|       1|                2|\n",
      "|            11|2019-08-22 03:09:26|         10|        770|    63.91|          11366|           0|       0|                0|\n",
      "|            12|2019-08-22 15:47:54|         10|          0|    58.89|          56874|           0|       0|                0|\n",
      "|            13|2019-08-22 21:59:20|         10|        649|    89.24|          79160|           0|       0|                0|\n",
      "|            14|2019-08-22 20:55:13|         11|        380|     9.89|          75313|           0|       0|                0|\n",
      "|            15|2019-08-22 16:39:03|         11|        337|    83.36|          59943|           0|       0|                0|\n",
      "|            16|2019-08-22 23:15:07|         11|        973|    35.12|          83707|           0|       0|                0|\n",
      "|            17|2019-08-22 07:39:45|         12|          9|     74.0|          27585|           0|       0|                0|\n",
      "|            18|2019-08-22 05:35:39|         12|        745|   108.63|          20139|           0|       0|                0|\n",
      "|            19|2019-08-22 10:29:16|         12|          9|    84.45|          37756|           0|       0|                0|\n",
      "+--------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f9f4dd-05f4-48d0-aa9d-85bfcb39f4d8",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ce5b182a-333a-4008-b050-e602513b99b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:======================================================>  (20 + 1) / 21]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество до препроцессинга: 46988418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Количество до препроцессинга:\", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "381f1dca-5e95-4192-a8a7-11401c007333",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.drop(\"any\") # очистка пустых значений\n",
    "df = df.dropDuplicates(['transaction_id']) # очистка дублей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b482032f-06b4-4425-b870-06b56dbebd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:====================================================> (193 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество после препроцессинга: 46988137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Количество после препроцессинга:\", df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815b7b88-e576-4713-a2fb-5c5b137f194a",
   "metadata": {},
   "source": [
    "## Запись в S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1c03473d-02f7-4c1e-adfe-1a3b9b852de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist_s3 = list(set([\n",
    "        key['Key'].split('.')[0]\n",
    "        for key in s3.list_objects(Bucket=bucket)['Contents']\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7a13ca75-5b60-4fb6-bb4a-655176b4cff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2019-09-21', '2019-08-22']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filelist_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2092fee4-7e1d-4ed5-8903-658d4cce40ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_exist(filename: str, bucket: str = RESULT_BUCKET) -> bool:\n",
    "    session = boto3.session.Session(aws_access_key_id=KEY_ID, aws_secret_access_key=SECRET)\n",
    "    s3 = session.client(\n",
    "        service_name='s3',\n",
    "        endpoint_url='https://storage.yandexcloud.net'\n",
    "    )\n",
    "    filelist_s3 = list(set([\n",
    "        key['Key'].split('.')[0]\n",
    "        for key in s3.list_objects(Bucket=bucket)['Contents']\n",
    "    ]))\n",
    "    if filename in filelist_s3:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d4ad8604-8c4f-4f40-823c-c683822962e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_file_exist(filename_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b56b97f7-6744-4bf8-a35e-e784bc780dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.repartition(1).write.parquet(f\"{target_s3}{fname_no_symbols}.parquet\", mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
